{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa17082-c6c9-4bf9-bab7-922660f4fb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, expr, split, trim, lower, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948d140f-812d-4dff-b741-2dc4401b92d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class invoiceStreamBatch:\n",
    "  def __init__(self):\n",
    "    self.base_data_dir = \"/FileStore/test\"\n",
    "  \n",
    "  def getSchema(self):\n",
    "    return '''InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "            CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint, \n",
    "            PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, \n",
    "            DeliveryType string,\n",
    "            DeliveryAddress struct<AddressLine string, City string, ContactNumber string, PinCode string, State string>,\n",
    "            InvoiceLineItems array<struct<ItemCode string, ItemDescription string, ItemPrice double, ItemQty bigint, TotalValue double>>'''\n",
    "  \n",
    "  def readInvoices(self):\n",
    "    return ( spark.readStream\n",
    "            .format(\"json\")\n",
    "            .schema(self.getSchema())\n",
    "            .option(\"inferSchema\", True)\n",
    "            .load(f\"{self.base_data_dir}/data/invoices\")\n",
    "            )\n",
    "    \n",
    "  def explodeInvoices(self, invoiceDF):\n",
    "    return ( invoiceDF.selectExpr(\"InvoiceNumber\", \"CreatedTime\", \"StoreID\", \"PosID\", \"CustomerType\", \"PaymentMethod\", \"DeliveryType\", \"DeliveryAddress.City\", \"DeliveryAddress.State\",\"DeliveryAddress.PinCode\", \"explode(InvoiceLineItems) as LineItem\")\n",
    "            )\n",
    "    \n",
    "  def flattenInvoices(self, explodedDF):\n",
    "    return ( explodedDF.withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\"))\n",
    "            .withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\"))\n",
    "            .withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\"))\n",
    "            .withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\"))\n",
    "            .withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\"))\n",
    "            .drop(\"LineItem\")\n",
    "          )\n",
    "    \n",
    "  def appendInvoices(self, flattenedInvoices, trigger=\"batch\"):\n",
    "    sQuery = ( flattenedInvoices.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", f\"{self.base_data_dir}/checkpoint/invoices\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"maxFilesPerTrigger\", 1)\n",
    "          )\n",
    "    if trigger == \"batch\":\n",
    "      return (sQuery.trigger(availableNow = True)\n",
    "               .toTable(\"invoice_line_items\")\n",
    "               )\n",
    "    else:\n",
    "      return (sQuery.trigger(processingTime = trigger)\n",
    "               .toTable(\"invoice_line_items\")\n",
    "               )\n",
    "    \n",
    "  def process(self, trigger='batch'):\n",
    "    print(\"Starting Invoice processing stream ...\")\n",
    "    invoicesDF = self.readInvoices()\n",
    "    explodedDF = self.explodeInvoices(invoicesDF)\n",
    "    resultDF = self.flattenInvoices(explodedDF)\n",
    "    sQuery = self.appendInvoices(resultDF, trigger)\n",
    "    print(\"Done\\n\")\n",
    "    return sQuery\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Streaming-Batch",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}