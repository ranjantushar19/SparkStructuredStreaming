{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c3822c-d168-41c1-9c23-ecfd3e90f0c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Streaming-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90ffb5e-13f3-49b4-a6ad-170d1049ff44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511d2be0-0695-487d-a885-dadc3d562867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class invoiceStreamBatchTestSuite:\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.base_data_dir = \"/FileStore/test\"\n",
    "  \n",
    "  def cleanTest(self):\n",
    "    print(\"Starting cleanup ...\")\n",
    "    # Drop delta table and hive tables.\n",
    "    spark.sql(\"drop table if exists invoice_line_items\")\n",
    "    dbutils.fs.rm(\"/user/hive/warehouse/invoice_line_items\", True)\n",
    "    # delete raw/staging data and checkpoint directory\n",
    "    dbutils.fs.rm(f\"{self.base_data_dir}/checkpoint/invoices\", True)\n",
    "    dbutils.fs.rm(f\"{self.base_data_dir}/data/invoices\", True)\n",
    "    # create raw/staging directory.\n",
    "    dbutils.fs.mkdirs(f\"{self.base_data_dir}/data/invoices\")\n",
    "    print(\"Done.\")\n",
    "  \n",
    "  def ingestData(self, itr):\n",
    "    print(f\"Starting ingestion of file #{itr}...\")\n",
    "    dbutils.fs.cp(f\"{self.base_data_dir}/invoices_{itr}.json\", f\"{self.base_data_dir}/data/invoices/\")\n",
    "    print(\"Done.\\n\")\n",
    "  \n",
    "  def assertResult(self, expected_count):\n",
    "    print(\"Starting validation ...\")\n",
    "    #print(f\"Testing first iteration of invoice stream ...\")\n",
    "    actual_count = spark.sql(\"select count(*) from invoice_line_items\").collect()[0][0]\n",
    "    assert actual_count == expected_count, f\"Test Failed! Actual count is {actual_count}\"\n",
    "    print(\"Done.\")\n",
    "    #print(\"Successfully tested first iteration of invoice stream.\\n\")\n",
    "\n",
    "  def waitForMicroBatch(self, sleep_time=40):\n",
    "    print(f\"Sleeping for {sleep_time} seconds ...\")\n",
    "    time.sleep(sleep_time)\n",
    "    print(f\"Woke up from {sleep_time} seconds sleep.\")\n",
    "  \n",
    "  def runStreamTest(self):\n",
    "    self.cleanTest()\n",
    "\n",
    "    iStream = invoiceStreamBatch()\n",
    "    streamQuery = iStream.process(\"30 seconds\")\n",
    "\n",
    "    print(\"Testing first iteration of invoice stream ...\")\n",
    "    self.ingestData(1)\n",
    "    self.waitForMicroBatch()\n",
    "    self.assertResult(1253)\n",
    "    print(\"First iteration of invoice stream completed.\\n\")\n",
    "\n",
    "    print(\"Testing second iteration of invoice stream ...\")\n",
    "    self.ingestData(2)\n",
    "    self.waitForMicroBatch()\n",
    "    self.assertResult(2510)\n",
    "    print(\"Third iteration of invoice stream completed.\\n\")\n",
    "\n",
    "    print(\"Testing third iteration of invoice stream ...\")\n",
    "    self.ingestData(3)\n",
    "    self.waitForMicroBatch()\n",
    "    self.assertResult(3994)\n",
    "    print(\"Third iteration of invoice stream completed.\\n\")\n",
    "\n",
    "    streamQuery.stop()\n",
    "\n",
    "  def runBatchTest(self):\n",
    "    self.cleanTest()\n",
    "\n",
    "    iStream = invoiceStreamBatch()\n",
    "    #streamQuery = iStream.process(\"30 seconds\")\n",
    "    # In batch mode, if it didn't find any data, streming handle will stop immediately.\n",
    "\n",
    "    print(\"Testing first batch of invoice stream ...\")\n",
    "    self.ingestData(1)\n",
    "    self.ingestData(2)\n",
    "    streamQuery = iStream.process(\"batch\")  #no need of stopping it as it will stop automatically.\n",
    "    self.waitForMicroBatch()\n",
    "    self.assertResult(2510)\n",
    "    print(\"First batch of invoice stream completed.\\n\")\n",
    "\n",
    "    print(\"Testing second batch of invoice stream ...\")\n",
    "    self.ingestData(3)\n",
    "    streamQuery = iStream.process(\"batch\")\n",
    "    self.waitForMicroBatch()\n",
    "    self.assertResult(3994)\n",
    "    print(\"Second batch of invoice stream completed.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "954d7a2e-5a9d-484d-b197-4269ff712e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleanup ...\nDone.\nStarting Invoice processing stream ...\nDone\n\nTesting first iteration of invoice stream ...\nStarting ingestion of file #1...\nDone.\n\nSleeping for 40 seconds ...\nWoke up from 40 seconds sleep.\nStarting validation ...\nDone.\nFirst iteration of invoice stream completed.\n\nTesting second iteration of invoice stream ...\nStarting ingestion of file #2...\nDone.\n\nSleeping for 40 seconds ...\nWoke up from 40 seconds sleep.\nStarting validation ...\nDone.\nThird iteration of invoice stream completed.\n\nTesting third iteration of invoice stream ...\nStarting ingestion of file #3...\nDone.\n\nSleeping for 40 seconds ...\nWoke up from 40 seconds sleep.\nStarting validation ...\nDone.\nThird iteration of invoice stream completed.\n\n"
     ]
    }
   ],
   "source": [
    "isTS = invoiceStreamBatchTestSuite()\n",
    "isTS.runStreamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb9b784-c131-4fe5-988c-8283b9be4924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleanup ...\nDone.\nTesting first batch of invoice stream ...\nStarting ingestion of file #1...\nDone.\n\nStarting ingestion of file #2...\nDone.\n\nStarting Invoice processing stream ...\nDone\n\nSleeping for 40 seconds ...\nWoke up from 40 seconds sleep.\nStarting validation ...\nDone.\nFirst batch of invoice stream completed.\n\nTesting second batch of invoice stream ...\nStarting ingestion of file #3...\nDone.\n\nStarting Invoice processing stream ...\nDone\n\nSleeping for 40 seconds ...\nWoke up from 40 seconds sleep.\nStarting validation ...\nDone.\nSecond batch of invoice stream completed.\n\n"
     ]
    }
   ],
   "source": [
    "isTS.runBatchTest()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Streaming-Batch-TestSuite",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}